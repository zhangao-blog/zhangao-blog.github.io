---
layout:     post
title:      Sora--文生视频
subtitle:   基于diffusion model的发展历程及其总体架构
date:       2024-08-23
author:     ZA
header-img: img/Note2.jpg
catalog: true
tags:
    - 大模型
    - 多模态
---

> Sora是一个AI模型，能够根据文本输入(故事、说明或命令)创造真实和富有想象力的视频输出。

![](https://s3.bmp.ovh/imgs/2024/09/04/02a4e10599ac7bcb.png)
它在AI文本到视频转换领域实现了 **质的飞跃** ，无论是在技术，还是在效果上。这体现在数据处理和视频生成技术的重大突破，以及AI在理解和创造视觉内容方面的巨大潜力。

## 目标：

创建一个能够理解复杂文本描述并将其转化为高质量视频内容的系统。并最终能够模拟真实世界的复杂互动和动态环境，实现一个“通用物理世界模拟器”

- 文本 ---> token ---> GPT的主要工作就是预测下一个token
- 视频 ---> Patch ---> Sora的主要工作就是预测下一个Patches

## 技术路线：

一、问题：**一个大的图片，AI如何来理解**？~大图切分，保留位置信息编码用于训练

[1]把一个大图拆分为多个面积相等的**小图像块，每个小块即是一个patch**。小图块易于被进一步序列化（面->线），从而展开得到一维向量，其中每个Patch还保留有它之前的位置信息。 将这些内容统一编码，就得到了**最终模型训练所需要的向量，一串数字**。
![](https://s3.bmp.ovh/imgs/2024/09/04/d527b2b8fd9122e5.png)
[1]An image is worth 16x16 words: Transformers for image recognition at scale[J]. arXiv preprint arXiv:2010.11929, 2020.（首次把Transformer应用于视觉领域）

二、问题：**上述工作只能处理正方形图片3*3的切分！不同宽高比分辨率怎么办？**

Navit[2]实现了不同宽高比分辨率图像的"Patchify"，也就是**图像块化**。此外，还引入了一个新机制：根据图像相似度，**丢弃掉雷同的Patch**，加快训练速度。
![](https://s3.bmp.ovh/imgs/2024/09/04/b995864345d73162.png)
Sora从该工作中收到了启发，视频 = 各种分辨率下的多张图片，且视频内容帧与帧之间往往是类似的。
[2]Patch n’pack: Navit, a vision transformer for any aspect ratio and resolution[J]. Advances in Neural Information Processing Systems, 2024, 36.（实现不同宽高分辨率的图像块化）

三、问题：**相较于原始图片，原始视频这么大怎么压缩？**

[3]的核心思想是从图像中提取关键信息，也就是最核心的浅空间特征。后续只对钱空间特征进行简便轻量的操作。最终还会根据这些浅空间特征，还原出一张新的图片。从而节省计算量
此时就建立的AI绘图的基础：“写关键词 -> 转换浅空间特征 -> 还原图片”。
![](https://s3.bmp.ovh/imgs/2024/09/04/a6478ad25098723d.png)
[3]High-resolution image synthesis with latent diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10684-10695.（AI画图的奠基）

四、**因此，Sora的实现，就基于工作二（Patch分割）和工作三（视频压缩）的组合**

1. 视频经过[3]压缩，变成低维度浅空间特征(Latent)；
2. 再对浅空间特征用[2]，进一步压缩为一个时空块(Patch)；
   这样各个时空块之间独立，海水是海水，蝴蝶是蝴蝶，背景和角色都可以随意替换。
   ![](https://s3.bmp.ovh/imgs/2024/09/04/293d089ce6ac4efa.png)
   这里的时空块主要参考了[4]的设计：
   ![](https://s3.bmp.ovh/imgs/2024/09/04/ea845b542cfe1d79.png)
   [4]Vivit: A video vision transformer[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 6836-6846.

五、**将上述数据整合，进入到Diffusion模型的训练阶段。**

训练思想主要基于[5]提出的DiT技术(Diffusion Transformers)，它使用OpenAI擅长的Transformer模型来替换原本扩散模型里的U-Net架构，实现更好的灵活性。
![](https://s3.bmp.ovh/imgs/2024/09/04/85b7e2ff95950309.png)
[5]Scalable diffusion models with transformers[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 4195-4205.（提出Diffusion + Transformer）

## 总体架构：

Sora的架构是一种结合了扩散模型和Transformer技术的创新设计。文本->Transformer+扩散模型->视频

- “扩散模型” 在Sora中用于逐步细化和完善视频帧；
- “Transformer” 则负责处理和解析复杂的文本输入；
  这种结合使得Sora不仅能够生成视觉上吸引人的视频，还能确保视频内容与文本描述紧密相连。

## 训练流程：

1. **视频数据收集与标注**：Sora的训练开始于收集大量视频数据，包括已标注和未标注视频。这为Sora学习和理解多样化视觉内容奠定了基础；
2. **标注补全**：对于未标注的视频，通过一个专门的图片字幕模型（基于DALLE3），观察视频内容并自动生成描述性文字，填补空白的标注信息；
3. **GPT-4优化描述**：Sora 借助 GPT-4 提高视频描述文字的质量，通过优化现有及和自动生成的标注，实现更加准确、丰富的描述效果；
4. **视频压缩**：通过视频压缩模型，视频经历“画面帧” -> “特征立方体” -> “Patch时序块”将处理后的视频切分为一个个小的视频块儿“Patches”。将视频块Patches压缩并转换到高效的潜在空间中，以简化学习流程；
5. **潜在空间处理**：潜在空间里的压缩数据，被作为训练素材用于新视频的生成。
6. **训练模型**：在潜在空间中、使用具有Transformer结构的扩散模型，进行训练；

- 训练输入：1.文本描述；2.添加随机噪音的视频块patches；
- 训练输出：视频在潜在空间中的表示。

7. **视频恢复**：在上述过程中同步得到的一个视频解码模型，用于从压缩格式恢复高清视频；
8. **资源支持**：整个训练过程依赖于强大的计算资源和硬件支持。这些资源是处理大规模视频数据和运行复杂模型Sora的关键。
   ![](https://s3.bmp.ovh/imgs/2024/09/05/468ce83f7eb2457a.png)
   Sora的核心是扩散变换器模型(DiT，Diffusion transformers)。它使用Transformer去替代传统UNet，可以看到中间的Latent Space中负责Denoising的生成模型替换成了Transfomers。
   传统文生视频基于潜在扩散模型（LDM，Latent diffusion models），如下图，中间Latent Space中负责Denoising的生成模型是是U-Net，U-Net通过一系列渐进的去噪步骤，将噪声数据逐步转成输出的最终结果，最终结果通过Decoder再映射回图片或者视频。
   ![](https://s3.bmp.ovh/imgs/2024/09/05/d23453b4c74b1111.png)

## 核心模块：

1. Transformer结构
   转换器(Transformer)是一种深度学习模型，最初被设计用于改进机器翻译任务。它被广泛应用于 Sora 的多个组件中，包括图片字幕模型、视频压缩模型，扩散模型。基本原理如下：
   1. Token/patch化: 通常，Transformer模型的第一步是将输入数据（文本、图像或视频）转换成一系列的tokens或patch。文本的tokens通常是单词或字符；图像的tokens可以是（一部分）图像块；视频的patches则是连续帧的一部分。这种转换实现了不同数据类型的统一处理。
   2. 嵌入层（Embedding Layer）: 转换后的tokens，经过一个嵌入层变为固定大小的向量，便于模型处理。对于文本tokens而言，这通常是将单词映射为高维空间中的点。
   3. 自注意力机制（Self-Attention）: 自注意力机制是Transformer的核心。这个机制使得模型能够评估序列中每一个token与其他tokens之间的重要性，从而学习tokens之间的关联，更好地理解整个序列
   4. 多头注意力（Multi-Head Attention）: 在这个阶段，模型不再是只从一个角度学习，而是同时从多个不同的角度学习tokens之间的关系。这帮助模型在不同的子空间中捕捉丰富的信息。
   5. 位置编码（Positional Encoding）: 由于Transformer不具备处理序列信息的能力，因此需要为每个token添加位置编码，以确保模型能够考虑到tokens的位置顺序。
   6. 编码器-解码器结构: Transformer结构包含两个部分，负责处理输入tokens的编码器、和根据编码器输出从而生成相应输出的解码器。这种结构非常适用于“序列到序列”的任务，例如机器翻译。
  




