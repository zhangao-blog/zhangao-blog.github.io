---
layout:     post
title:      机器学习(一) - 知识总结
subtitle:   激活函数及常见机器学习算法
date:       2023-12-20
author:     ZA
header-img: img/Note2.jpg
catalog: true
tags:
    - 机器学习
---

## 机器学习的主要任务

分类和回归属于监督学习，之所以称之为监督学习，是因为这类算法必须知道预测什么，即目标变量的分类信息。与监督学习相对应的是无监督学习，**其目标是从未标记的数据中发现隐藏的结构或模式。**此时数据没有类别信息，也不会给定目标值。在无监督学习中，将数据集合分成由类似的对象组成的多个类的过程被称为**聚类**；将寻找描述数据统计值的过程称之为**密度估计**。  


## 激活函数

###  **Sigmoid 激活函数**

对于向量的每一个数执行![image-20240331205058654](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240331205058654.png)

![image-20240229162928810](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240229162928810.png)

什么情况下适合使用 Sigmoid 激活函数呢？

- Sigmoid 函数的输出范围是 0 到 1。由于输出值限定在 0 到 1，因此它对每个神经元的输出进行了归一化；
- 用于将预测概率作为输出的模型。由于概率的取值范围是 0 到 1，因此 Sigmoid 函数非常合适；
- 梯度平滑，避免「跳跃」的输出值；
- 函数是可微的。这意味着可以找到任意两个点的 sigmoid 曲线的斜率；

缺点：

- 倾向于梯度消失；
- 函数输出不是以 0 为中心的，这会降低权重更新的效率；

###  **Tanh / 双曲正切激活函数**

![image-20240229163020588](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240229163020588.png)

对于向量的每一个数执行正切。

优势：

- 首先，当输入较大或较小时，输出几乎是平滑的并且梯度较小，这不利于权重更新。二者的区别在于输出间隔，tanh 的输出间隔为 1，并且整个函数以 0 为中心，比 sigmoid 函数更好；
- 在 tanh 图中，负输入将被强映射为负，而零输入被映射为接近零。

在一般的二元分类问题中，tanh 函数用于隐藏层，而 sigmoid 函数用于输出层，但这并不是固定的，需要根据特定问题进行调整。

### **ReLU 激活函数**

![image-20240229163131347](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240229163131347.png)

给定一个输入向量  ${z=(z_1,z_2,...z_n)}$，ReLU函数将每个元素${z_i}$转换为以下形式：

${ReLU(z_i)=max⁡(0,z_i)}$

优势：

- 当输入为正时，不存在梯度饱和问题。
- 计算速度快得多。ReLU 函数中只存在线性关系，因此它的计算速度比 sigmoid 和 tanh 更快。

缺点：

- Dead ReLU 问题。当输入为负时，ReLU 完全失效，在正向传播过程中，这不是问题。有些区域很敏感，有些则不敏感。但是在反向传播过程中，如果输入负数，则梯度将完全为零，sigmoid 函数和 tanh 函数也具有相同的问题；
- 我们发现 ReLU 函数的输出为 0 或正数，这意味着 ReLU 函数不是以 0 为中心的函数。

### Softmax激活函数

它通常用于多分类问题中，将原始的线性输出转换为归一化的概率分布。Softmax函数的定义如下：

给定一个具有 n 个实数值的向量 ${z=(z_1,z_2,...z_n)}$，Softmax 函数将每个元素${z_i}$ 转换为一个介于 0 到 1 之间的值，表示其所占总和的比例，并且这些值的总和为 1。具体来说，Softmax 函数的输出 σ*(**z**) 由以下公式给出：![](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240331204518321.png)

其中，*e* 是自然常数（欧拉数），σ*(**z**)*i* 表示经 Softmax 转换后的第 i* 个元素的值。

Softmax函数的特点是它对输入进行归一化，使得输出值之间可以被看作概率，因此在多分类问题中非常有用。它还具有平滑性和可微性的特点，因此在神经网络中广泛使用。

![image-20240331204339790](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240331204339790.png)

## Q：神经网络每一层都需要激活函数吗？

在实践中，激活函数通常被应用在隐藏层和输出层，而输入层通常不需要使用激活函数。激活函数主要用于引入非线性，增加神经网络的表达能力，因此它们通常被应用在隐藏层和输出层。

## 反向传播

反向传播过程：

1. **前向传播**：在反向传播开始之前，首先进行前向传播。在前向传播过程中，输入数据被送入网络，通过层层的处理（包括权重相乘和激活函数的应用），最终产生输出。

2. **计算误差**：计算输出层的预测值与实际值之间的误差。这个误差通常通过一个损失函数（如均方误差或交叉熵损失）来计算。

3. **反向传播误差**：反向传播的核心步骤是将输出误差反向传播到网络的每一层。这是通过链式法则计算每层权重对损失函数的偏导数（梯度）来实现的。简而言之，我们计算每个权重如何影响最终的误差，然后根据这个影响来调整权重。

   1. 反向传播会从最后一层向前计算，最后一层是损失函数对于权重最高的点就是对损失函数求权重的偏导，根据损失函数对于输出值的导数，即<img src="https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240229165759431.png" alt="image-20240229165759431" style="zoom:50%;" />		<img src="https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240229165903504.png" alt="image-20240229165903504" style="zoom:50%;" />

      ​	然后根据链式的求导法则我们就可以求出损失函数对于这个神经网络权重的梯度也就是导数，然后我们把最大的梯度求出来就可以用于梯度更新，然后前边的每一层这么计算都可以进行梯度更新。

      https://www.bilibili.com/video/BV1yG411x7Cc/?spm_id_from=333.337.search-card.all.click&vd_source=d9cf70e111cd00a30678767b6fba700c

4. **权重更新**：一旦计算出网络中所有权重的梯度，就使用这些梯度来更新权重。这通常通过一个简单的规则完成，例如权重的新值等于旧值减去学习率乘以梯度（这里的学习率是一个小的正数，用来控制学习的步长）。

5. **迭代**：重复上述过程（前向传播、计算误差、反向传播误差、权重更新）多次，每次迭代旨在进一步减小网络的损失函数，直到网络的性能达到满意的水平。

## 逻辑回归

交叉熵损失函数用于多分类，对数损失函数用于二分类问题。

<img src="https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240229172213267.png" alt="image-20240229172213267" style="zoom:200%;" />

![image-20240229172218530](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240229172218530.png)

![](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240320184915686.png)

### 逻辑回归优缺点：

**优点**

1. **简单而高效**：逻辑回归是一种相对简单的分类算法，易于实现和理解。训练速度快，适用于处理大规模数据集。
2. **可解释性强**：逻辑回归输出的结果可以很直观地解释为概率，能够提供对分类结果的理解，例如特征的影响程度等。
3. **抗噪声能力较强**：逻辑回归对于特征之间的相关性不敏感，可以处理一定程度的噪声和缺失数据。

**缺点**

1. **处理非线性关系困难**：逻辑回归假设特征与目标变量之间的关系是线性的，对于非线性关系的数据拟合能力较弱。
2. **容易受到共线性影响**：当特征之间存在高度相关性时，逻辑回归模型的性能可能下降，需要进行特征选择或正则化处理。
3. **不适用于复杂数据**：逻辑回归很难捕捉数据中复杂的关系和交互作用，因此在处理复杂数据时效果可能不佳。

概率空间内的逻辑回归就是对数几率空间内的线性回归，

参数模型是一种假设模型的方法，其中模型的结构是事先定义好的，但模型的参数需要从数据中进行估计。换句话说，参数模型假设数据遵循某种特定的分布或模式，并且该模式的参数是未知的，需要从数据中学习或估计得到。举例来说，线性回归是一个典型的参数模型。在线性回归中，假设目标变量与特征之间存在线性关系，模型的结构已经确定为线性。但是线性的系数需要训练计算

##  怎么解决欠拟合

- 增加模型复杂度
- 换非线性的模型
- 更改超参数

## 怎么解决过拟合（重点）

- 获取和使用更多的数据（数据集增强）——解决过拟合的根本性方法 

- 特征降维:人工选择保留特征的方法对特征进行降维

- 加入正则化，控制模型的复杂度

- **Dropout**

- Early stopping

- 交叉验证
- **增加噪声**

## 无监督-决策树

本质上决策树是通过一系列规则对数据进行分类的过程。

决策树主要包括**三个部分：内部节点、叶节点、边。内部节点是划分的特征，边代表划分的条件，叶节点表示类别。**k-近邻算法可以完成很多分类任务，但是它最大的缺点就是无法给出数据的内在含义，决策树的主要优势就在于数据形式非常容易理解  

基尼系数（Gini coefficient）通常用于衡量一个数据集的纯度或不纯度。当下标准的不确定程度**选择基尼系数最小**的作为分类的标准

![image-20240301161629120](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240301161629120.png)

![image-20240306162800963](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240306162800963.png)

决策树过程

1.对于一个给定的特征，将数据集按照该特征的取值进行划分，得到若干个子集。（数据集拥有若干特征）

加权**基尼系数**最小的特征作为当前节点的分裂特征。



![image-20240313165131461](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240313165131461.png)

 ![image-20240313165042879](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240313165042879.png)

决策树中信息增益的计算过程涉及到熵(Entropy)的概念。信息增益用于衡量在特定条件下对系统带来的混乱度减少程度，即划分前后熵的变化。下面是信息增益的计算过程：

1. **计算初始熵（Entropy）：** 针对数据集的目标变量，计算其初始熵。熵的计算公式为： $Entropy(S)=-\sum_{i=1}^np_i\log_2(p_i)$其中，pi是目标变量每个类别的概率，n* 是类别的总数。
2. **对数据集进行划分：** 以某个特征对数据集进行划分，生成多个子集。
3. **计算每个子集的熵：** 针对每个划分后的子集，计算其熵。
4. **计算加权平均熵：** 将每个子集的熵乘以相应子集所占比例的权重，并将结果相加，得到加权平均熵。
5. **计算信息增益：** 初始熵减去加权平均熵，即为信息增益。信息增益越大，说明划分后的子集纯度提升的程度越高。

信息增益（Information Gain）和基尼系数（Gini Impurity）都是用于决策树算法中的指标，用于选择最佳的特征进行节点划分。它们之间的关系如下：

1. **信息增益**：
   - 信息增益是根据信息论中的熵（Entropy）来计算的，熵用于衡量数据的不确定性。在决策树中，信息增益是指在划分数据集前后，熵的减少程度，即划分前后数据的纯度提高的程度。
   - 计算信息增益的步骤包括：首先计算划分前数据集的熵，然后对于每个可能的特征，计算其对数据集的加权平均熵，最后用划分前的熵减去加权平均熵，得到信息增益最大的特征就是最佳的划分特征。
   - 信息增益越大表示使用该特征进行划分能够带来更大的数据纯度提升，因此更有利于构建决策树。
2. **基尼系数**：
   - 基尼系数是用于衡量数据集的不纯度的指标，它表示从数据集中随机抽取两个样本，其类别标签不一致的概率。
   - 计算基尼系数的步骤包括：对于每个可能的特征和每个特征可能的取值，计算其在当前数据集上的基尼系数，然后对每个特征的基尼系数进行加权平均，选择基尼系数最小的特征作为最佳划分特征。
   - 基尼系数越小表示数据的纯度越高，因此在构建决策树时，选择基尼系数较小的特征进行划分更有利于提高节点的纯度。

https://www.bilibili.com/video/BV1ar4y137GD/?spm_id_from=333.999.0.0

## 监督方法-KNN算法

优点：精度高、对异常值不敏感、无数据输入假定。

缺点：计算复杂度高、空间复杂度高。适用数据范围：数值型和标称型。

### KD树

KD树可以在样本搜索时进行高效的查找，对于一维空间的切分的最终结果实际上切出的是线段，同理，二维空间中，对于该6个点集最后的切分结果的空间应该是子平面集合。这里，kd树最终目标就是构建一棵树，能够将二维平面切分得到的最终的子平面集，如下图所示：。一般而言，选择该**维度上的中位数对应的点作为**划分节点。通过搜索的方式进行查询

一旦KD树被构建，当有一个新的待分类样本到来时，我们可以利用KD树来找到它的K个最近邻居。这个过程通常分为两个步骤：

1. **搜索阶段**：从KD树的根节点开始，根据待分类样本的特征值，沿着树向下遍历，找到包含待分类样本的叶子节点。这个过程类似于在树中搜索一个元素的过程。
2. **回溯阶段**：从找到的叶子节点开始，向上回溯到根节点。在回溯的过程中，计算待分类样本与当前节点的距离，更新K个最近邻居的集合。如果发现更近的邻居，就更新K个最近邻居的集合。

![image-20240306161622923](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240306161622923.png)

## 朴素贝叶斯

朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），

优点：在数据较少的情况下仍然有效，可以处理多类别问题。缺点：对于输入数据的准备方式较为敏感。适用数据类型：标称型数据。

贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已知P(x|c)，要求P(c|x)，那么可以使用下面的计算方法：  

$p(c|x)=\frac{p(x|c)p(c)}{p(x)}$​

生成模型（Generative Model）和判别模型（Discriminative Model）是机器学习中常见的两种建模方式，它们的主要区别在于模型学习的目标和所建模拟合的内容：

1. **生成模型**：
   - 生成模型试图对联合概率分布 $P(X,Y)$ 进行建模，其中 $X$ 是输入特征，$Y$ 是对应的标签或输出。
   - 生成模型关注的是如何从特征生成样本数据，即它学习的是样本的生成过程。
   - 典型的生成模型包括朴素贝叶斯、隐马尔可夫模型（HMM）、生成对抗网络（GAN）等。
   - 生成模型通常能够生成新的样本，因为它们对数据的生成过程有一个明确的建模，因此可以从学习的分布中采样新的样本。
2. **判别模型**：
   - 判别模型试图学习条件概率分布 $P(Y|X)$，即在给定输入特征 $X$ 的情况下，预测对应的标签 $Y$。
   - 判别模型关注的是在已知特征条件下，预测标签的概率分布，而不是关注样本的生成过程。
   - 典型的判别模型包括逻辑回归、支持向量机（SVM）、决策树、神经网络等。
   - 判别模型通常更简单且直接，因为它们只需学习条件概率，而不需要对样本的生成过程进行建模。

主要区别总结如下：

- 生成模型关注样本的生成过程，学习联合概率分布 $P(X,Y)$，并能够生成新的样本。
- 判别模型关注在给定特征条件下预测标签的概率分布 $P(Y|X)$，不涉及样本的生成过程。
- 生成模型通常更复杂，因为它们需要对数据的整个联合分布进行建模。
- 判别模型通常更简单，因为它们只需要学习条件概率分布，不需要对数据的整个联合分布进行建模。

贝叶斯的训练过程：

![image-20240306190549398](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240306190549398.png)

## Logistic 回归  

优点：计算代价不高，易于理解和实现。

缺点：容易欠拟合，分类精度可能不高。

适用数据类型：数值型和标称型数据。

## SVM

定义：支持向量机是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法创建一个将新的实例分配给两个类别之一的模型，使其成为非概率二元**线性分类器**。

<img src="https://pic3.zhimg.com/80/v2-0d6eb1459f72940dc932795f345bae06_720w.webp" style="zoom: 50%;" />

从上图中选择最优的分类线，我们肯定会选择H3（H2的泛化能力弱）。对于支持向量机来说，数据点若是n维向量，我们用n−1维的超平面来分开这些点。但是可能有许多超平面可以把数据分类。最佳超平面的一个合理选择就是以最大间隔把两个类分开的超平面。因此，SVM选择能够使离超平面最近的数据点的到超平面距离最大的超平面。

1. **定义超平面**：在SVM中，超平面是一个d维空间中的(d-1)维子空间。对于二维空间，超平面是一条直线；对于三维空间，超平面是一个平面；以此类推。超平面可以用方程表示为： ${w*x+b=0}$其中， ${w}$是超平面的法向量，${b}$是截距， ${x}$是数据点。
2. **计算间隔**：对于每个训练样本，计算它到超平面的距离。这个距离被称为间隔。为了最大化间隔，SVM的目标是找到能够使得所有样本点到超平面的距离之和最大的超平面。
3. **最大化间隔**：SVM的优化目标是最大化间隔，同时确保所有样本点都被正确分类。通常情况下，这个优化问题可以通过求解凸二次规划问题来实现。

SVM的类别：

- 线性可分SVM

> 当训练数据线性可分时，通过硬间隔(hard margin，什么是硬、软间隔下面会讲)最大化可以学习得到一个线性分类器，即硬间隔SVM，如上图的的H3。

- 线性SVM

> 当训练数据不能线性可分但是可以近似线性可分时，通过软间隔(soft margin)最大化也可以学习到一个线性分类器，即软间隔SVM。

- 非线性SVM

> 当训练数据线性不可分时，通过使用核技巧(kernel trick)和软间隔最大化，可以学习到一个非线性SVM。

线性可分SVM

考虑如下形式的**线性可分**的训练数据集:

${(X_1,y_1),(X_2,y_2),(X_3,y_3)...(X_n,y_n)}$

其中${X_i}$是一个含有d个元素的列向量, 即${X_i∈R^d}$;是标量,${y_i∈+1,−1}$+1时表示正类别,−1时表示负类别.

一个超平面由法向量${X^T}$和截距b决定,其方程为$X^{T}W+b=0,$, 可以规定法向量指向的一侧为正类,另一侧为负类。下图画出了三个平行的超平面，法方向取左上方向。线性可分支持向量机利用**间隔最大化**求最优分离超平面,这时解是唯一的。

![](https://pic1.zhimg.com/80/v2-bfc8c57f522ce521994035859fcb388c_720w.webp)

将高数里面求两条平行直线的距离公式推广到高维可求得图中margin的

![image-20240307201014352](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240307201014352.png)

在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的数据点称为支持向量(support vector)



![](https://pic2.zhimg.com/80/v2-f9e1e7fd08460a5fab044c71ed8b0bb1_720w.webp)



将原始问题(primal problem), 可以应用拉格朗日乘子法构造拉格朗日函数(Lagrange function)再通过求解其对偶问题(dual problem)得到原始问题的最优解。最后求得

![image-20240307202944244](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240307202944244.png)



![image-20240307203009671](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240307203009671.png)

非线性：

如下图所示，核技巧的基本思路分为两步:*使用一个变换将原空间的数据映射到新空间(例如更高维甚至无穷维的空间)；*然后在新空间里用线性方法从训练数据中学习得到模型。![](https://pic1.zhimg.com/80/v2-87d10648809c0b6c5e473bd4565c2c08_720w.webp)

https://zhuanlan.zhihu.com/p/49331510



# 极大似然估计的原因

极大似然估计的估计是**参数值**${θ}$，举个例子，当我们估计出正面的概率时也就估计出了是哪个硬币，所以我们

![image-20240410092239398](C:\Users\97265\AppData\Roaming\Typora\typora-user-images\image-20240410092239398.png)

上式的未知量其实就是概率我们将概率转换为参数$p(x_1;\theta)\cdot p(x_2;\theta)\cdot p(x_3;\theta)\cdot\ldots\cdot p(x_n;\theta)$​，让这些概率相乘就是最后结果出现的概率

简化成$L(\theta)=L(x_1,x_2,x_3,\ldots,x_n;\theta)=\prod_{i=1}^np(x_i;\theta)$当  $\theta=\theta_{0}$时，似然函数 $L(x_1,x_2,x_3,\ldots,x_n;\theta_0)$的取值为0或趋近于0，那么意味着采用这个概率时 样本出现的概率为0时，，即压根儿不可能得到这一组样本值，或可能性非常非常小，那么你肯定不会觉得参数 $\theta$应该取   $\theta_{0}$这个数。所以我们为了尽可能的取到真正的参数 $\theta$我们就需要最大化$L(\theta)$，如何最大化就是求导数为0的点。然后就是由于这个连乘的函数求导数比较复杂，我们就将它转换为ln的形式。然后求导

极大似然估计的目的：我们估计出这个参数之后我们就可以以最大的概率以输入获得这个标签值，所以极大似然的目标是最大化函数值，



参考链接：https://www.zhihu.com/question/266629768

**逻辑回归离散化的原因**

离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展。离散后的特征对异常值更具鲁棒性，如age>30为1否则为0，对于年龄为200的也不会对模型造成很大的干扰。

逻辑回归引入非线性化特征

1.离散化

2.特征交叉，**对特征进行相乘**：将两个特征进行相乘，得到一个新的特征。例如，如果有两个特征 *x*1 和 x*2，可以创建一个新的特征 x*1×*x*2。

特征不平衡时解决办法

1. **过采样（Oversampling）**： 过采样是指增加少数类样本的数量，以使得数据集中各个类别的样本数量相对平衡。通常，过采样方法包括复制少数类样本、生成人工样本等。常见的过采样方法包括SMOTE（Synthetic Minority Over-sampling Technique）和ADASYN（Adaptive Synthetic Sampling）。过采样的优点是能够增加少数类样本的影响，提高模型对少数类的识别能力，缺点是可能导致模型过拟合，因为样本类别不平衡的情况没有得到真正解决。
2. **负采样（Undersampling）**： 负采样是指减少多数类样本的数量，以使得数据集中各个类别的样本数量相对平衡。通常，负采样方法包括删除多数类样本、随机下采样等。负采样的优点是能够减少多数类样本的影响，缺点是可能会丢失多数类样本的重要信息，导致模型在识别多数类样本时性能下降。

SVM相比LR更加适合小样本

# 优化器

**梯度下降法：**

​    随机梯度下降

​     批量梯度下降

计算方法：

1.计算梯度

2.求梯度的平均值

3.根据固定的学习率更新权重

优点：

算法简洁学习率恰当就可以收敛到最优

缺点

梯度下降法对学习率敏感

在平坦的地区因为梯度为0所以可以认为到达了极值点，提前结束陷入局部最小值

![image-20240418215704851](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240418215704851.png)

**动量梯度下降（Momentum）**

思想：让梯度具有惯性，每一次更新都是前边梯度的累加和当前梯度的加和

公式：累计梯度${v=av+(1-a)g}$这里v代表累计梯度，g代表当前梯度，

梯度更新：${x=x-ηv}$​    η是学习率

优点：

加快收敛

跳出局部最小值

**Adguard自适应学习率**

![image-20240418220327530](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240418220327530.png)



如果梯度一直很大，那么学习率就会变小防止震荡，如果梯度一直很小，那么学习率就会增加以更快的探索

**RMSProp（Root Mean Square Propagation）均方根传播算法**

![image-20240418220646267](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240418220646267.png)

**Adam（Adaptive Moment Estimation）自适应矩估计优化器**



![image-20240418220847146](https://imgs-1304492658.cos.ap-beijing.myqcloud.com/imgs/image-20240418220847146.png)





参考链接：https://www.bilibili.com/video/BV1jh4y1q7ua/?vd_source=d9cf70e111cd00a30678767b6fba700c
